# -*- coding: utf-8 -*-
"""sentiment_analysis_nlp_iu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/186610qsXk88DSu_ptxgCcP4HRbcUn1JT
"""

import os
import tarfile
import urllib.request
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
from glob import glob

# Download NLTK resources (run once)
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Step 1: Collect data (download and extract if not present)
def download_and_extract_dataset():
    url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
    filename = "aclImdb_v1.tar.gz"
    if not os.path.exists("aclImdb"):
        print("Downloading dataset...")
        urllib.request.urlretrieve(url, filename)
        print("Extracting dataset...")
        with tarfile.open(filename) as tar:
            tar.extractall()
        os.remove(filename)  # Clean up tar file
    else:
        print("Dataset already extracted.")

# Step 2: Load data from folders (with optional limit for subsampling)
def load_data(directory, limit=None):
    """
    Loads reviews and labels from pos/neg folders.
    - directory: 'aclImdb/train' or 'aclImdb/test'
    - limit: Total samples to load (balanced across pos/neg), or None for all.
    """
    reviews = []
    labels = []
    for label in ['pos', 'neg']:
        folder = os.path.join(directory, label)
        files = glob(os.path.join(folder, '*.txt'))
        if limit:
            files = files[:limit // 2]  # Balance pos/neg
        for file in files:
            with open(file, 'r', encoding='utf-8') as f:
                reviews.append(f.read())
            labels.append(1 if label == 'pos' else 0)
    return reviews, np.array(labels)

# Step 3: Preprocess text
def preprocess(reviews):
    """
    Preprocesses reviews: lowercase, remove stop words, lemmatize.
    Returns list of processed strings.
    """
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    processed = []
    for review in reviews:
        # Lowercase and tokenize (simple split for efficiency)
        words = review.lower().split()
        # Remove stop words and lemmatize
        words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]
        processed.append(' '.join(words))
    return processed

# Main pipeline
def run_sentiment_analysis(limit=None):
    # Download and load data
    download_and_extract_dataset()
    train_reviews, train_labels = load_data('aclImdb/train', limit=limit)
    test_reviews, test_labels = load_data('aclImdb/test', limit=limit)

    print(f"Loaded {len(train_reviews)} train samples and {len(test_reviews)} test samples.")

    # Preprocess
    train_processed = preprocess(train_reviews)
    test_processed = preprocess(test_reviews)

    # Encode: TF-IDF Vectorization
    vectorizer = TfidfVectorizer(max_features=5000)  # Limit features for efficiency
    X_train = vectorizer.fit_transform(train_processed)
    X_test = vectorizer.transform(test_processed)

    # Train model: Multinomial Naive Bayes
    model = MultinomialNB()
    model.fit(X_train, train_labels)

    # Evaluate
    predictions = model.predict(X_test)
    accuracy = accuracy_score(test_labels, predictions)
    report = classification_report(test_labels, predictions, target_names=['Negative', 'Positive'])

    print(f"\nAccuracy: {accuracy:.4f}")
    print("\nClassification Report:\n", report)

    return model, vectorizer

# Function to analyze a new review (after training)
def analyze_review(model, vectorizer, new_review):
    processed = preprocess([new_review])
    X_new = vectorizer.transform(processed)
    prediction = model.predict(X_new)[0]
    return "Positive" if prediction == 1 else "Negative"

# Run the pipeline (e.g., full dataset: limit=None; small: limit=100)
if __name__ == "__main__":
    # Example: Run on full dataset
    model, vectorizer = run_sentiment_analysis(limit=None)

    # Example analysis on a new review
    new_review = "This movie was absolutely fantastic! I loved every minute of it."
    sentiment = analyze_review(model, vectorizer, new_review)
    print(f"\nSentiment for new review: {sentiment}")

    # For evaluation in different scenarios (run separately with different limits):
    # - Small: run_sentiment_analysis(limit=100)  # ~50 pos/neg each
    # - Medium: run_sentiment_analysis(limit=1000)
    # - Large: run_sentiment_analysis(limit=10000)
    # - Full: run_sentiment_analysis(limit=None)  # Expected accuracy ~85-88%